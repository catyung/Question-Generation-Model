{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5baded2",
   "metadata": {},
   "source": [
    "## Train - Question Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acaeb304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 06:26:51.280075: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-30 06:26:51.468000: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-30 06:26:52.416086: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-30 06:26:52.416192: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-30 06:26:52.416210: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'T5PegasusTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    MT5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from utils import T5PegasusTokenizer\n",
    "from transformers.models.mt5.modeling_mt5 import MT5ForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_path = 'imxly/t5-pegasus'\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "set_seed(42)\n",
    "\n",
    "t5_model = MT5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "tokenizer = T5PegasusTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b2d5ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xquad (/home/jupyter-daniel/.cache/huggingface/datasets/xquad/xquad.zh/1.0.0/39e1ff0497cbbfb79bbff61024031c10872bbd7c4fd8bc250207a965c39d3336)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ef6c3176554db090314b63ae012c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"xquad\",\"xquad.zh\")\n",
    "data = data['validation']\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "data['question'] = data['question']\n",
    "data['context'] = data['context']\n",
    "data['answers'] = list(map(lambda x: x['text'][0], data['answers']))\n",
    "\n",
    "data['input'] = 'question: '+'<answer>' + data['answers'] + '<context>' + data['context']\n",
    "data['label'] = data['question']\n",
    "\n",
    "input_data = list(zip(data['input'],data['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c8c0e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-daniel/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in t5_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in t5_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-4, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c7e3395",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/jupyter-daniel/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.772 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1\n",
      "epoch  2\n",
      "epoch  3\n",
      "epoch  4\n",
      "epoch  5\n",
      "epoch  6\n",
      "epoch  7\n",
      "epoch  8\n",
      "epoch  9\n"
     ]
    }
   ],
   "source": [
    "mt5_model.train()\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  print (\"epoch \",epoch)\n",
    "  for input,output in input_data:\n",
    "    input_sent = \"qa-generation :\"+input+ \"</s> \"\n",
    "    ouput_sent = output+\"</s>  \"\n",
    "\n",
    "    tokenized_inp = tokenizer.encode_plus(input_sent,  max_length=512, pad_to_max_length=True,return_tensors=\"pt\")\n",
    "    tokenized_output = tokenizer.encode_plus(ouput_sent, max_length=100, pad_to_max_length=True,return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "    input_ids  = tokenized_inp[\"input_ids\"].to(device)\n",
    "    attention_mask = tokenized_inp[\"attention_mask\"].to(device)\n",
    "\n",
    "    lm_labels= tokenized_output[\"input_ids\"].to(device)\n",
    "    decoder_attention_mask=  tokenized_output[\"attention_mask\"].to(device)\n",
    "\n",
    "    # the forward function automatically creates the correct decoder_input_ids\n",
    "    output = mt5_model(input_ids=input_ids, labels=lm_labels,decoder_attention_mask=decoder_attention_mask,attention_mask=attention_mask)\n",
    "    loss = output[0]\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model & tokenizer\n",
    "mt5_model.save_pretrained(\"final-mt5\")\n",
    "tokenizer.save_pretrained(\"final-mt5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eef8c08",
   "metadata": {},
   "source": [
    "## Answer Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed7808b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 07:10:40.768281: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-02 07:10:41.676820: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-02 07:10:43.341816: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-02 07:10:43.341974: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-02 07:10:43.341986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from utils import T5PegasusTokenizer\n",
    "from transformers.models.mt5.modeling_mt5 import MT5ForConditionalGeneration\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import jieba\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    MT5ForConditionalGeneration,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "def answer_generation(context,answer=\"\",lang=\"\") :\n",
    "\n",
    "    # Manual Setting the Answer \n",
    "\n",
    "    if answer == \"\" :\n",
    "        if lang == \"chi\":\n",
    "        # Generate Answer from KeyBERT \n",
    "            context = \" \".join(jieba.cut(context))\n",
    "\n",
    "        sentence_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        kw_model = KeyBERT(model=sentence_model)\n",
    "        keywords = kw_model.extract_keywords(context,keyphrase_ngram_range=(1,2),use_mmr=True,diversity=0.9,top_n=3)\n",
    "\n",
    "        answer_list = []\n",
    "\n",
    "        for keyword in keywords :\n",
    "            answer_list.append(keyword[0])\n",
    "    else :\n",
    "        answer_list = []\n",
    "        answer_list = [answer]\n",
    "    return context,answer_list \n",
    "\n",
    "def question_generation(context,answer_list:list,lang=\"\") :\n",
    "    \n",
    "    if lang ==\"en\":\n",
    "        eng_model_path = \"iarfmoose/t5-base-question-generator\"\n",
    "        model = T5ForConditionalGeneration.from_pretrained(eng_model_path).to(device)\n",
    "        tokenizer = T5Tokenizer.from_pretrained(eng_model_path)\n",
    "        \n",
    "    else :\n",
    "        chi_model_path = \"final-mt5\"\n",
    "        model = MT5ForConditionalGeneration.from_pretrained(chi_model_path).to(device)\n",
    "        tokenizer = T5PegasusTokenizer.from_pretrained(chi_model_path)\n",
    "    \n",
    "    # Generate Question based on answer     \n",
    "    \n",
    "    output_list = {\"question\":[],\"answer\":[]}\n",
    "\n",
    "    for i in range(len(answer_list)) :\n",
    "        format_input = \"qa-question: \" + \"<answer>\" + answer_list[i] + '<context>' + context\n",
    "\n",
    "        input_ids = tokenizer.encode(format_input, return_tensors='pt').to(device)\n",
    "        output = model.generate(input_ids,\n",
    "                            decoder_start_token_id=tokenizer.cls_token_id, #101\n",
    "                            eos_token_id=tokenizer.sep_token_id, #102\n",
    "                            max_length=64)\n",
    "        \n",
    "        result = tokenizer.decode(output[0])\n",
    "        result = result.replace(\"<pad> \",\"\")\n",
    "        result = result.replace(\"</s>\",\"\")\n",
    "        \n",
    "        if lang==\"chi\":\n",
    "            result = ''.join(result).replace(' ', '')\n",
    "            result = result.replace(\"[CLS]\",\"\")\n",
    "            result = result.replace(\"</s>[SEP]\",\"\")\n",
    "        \n",
    "        output_list['question'].append(result)\n",
    "        output_list['answer'].append(answer_list[i])\n",
    "        \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfe8373f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is the name of the company???????????????...</td>\n",
       "      <td>wistron corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the name of the company??</td>\n",
       "      <td>desktop computers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When was Wistron Corporation spun off??</td>\n",
       "      <td>spun 2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question               answer\n",
       "0  what is the name of the company???????????????...  wistron corporation\n",
       "1                  What is the name of the company??    desktop computers\n",
       "2            When was Wistron Corporation spun off??            spun 2000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "eng_context = \"Wistron Corporation is an electronics manufacturer based in Taiwan. It was the manufacturing arm of Acer Inc. before being spun off in 2000. As an original design manufacturer, the company designs and manufactures products for other companies to sell under their brand name. Wistron products include notebook and desktop computers, servers, storage, LCD TVs, handheld devices, and devices and equipment for medical applications.\"\n",
    "chi_context = \"\"\"\n",
    "纬创资通股份有限公司，简称纬创，是一家ODM企业，于2001年由宏碁拆分出来，营运总部位于台湾。纬创资通是全球资讯产品主要供应商之一，全球员工逾80,000名。主要产品包括可携式电脑系统、桌上型电脑系统、伺服器及网路储存设备、资讯家电、通讯产品、云端及绿资源技术。\"\"\"\n",
    "\n",
    "context,ans = answer_generation(eng_context,answer=\"\",lang=\"en\")\n",
    "\n",
    "result_dict = question_generation(context,ans,lang=\"en\")\n",
    "\n",
    "qa_df = pd.DataFrame(result_dict)\n",
    "\n",
    "qa_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
